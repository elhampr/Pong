from lib import wrappers
from lib import dqn_model

import argparse
import time
import numpy as np
import collections

import torch
import torch.nn as nn
import torch.optim as optim
from tensorboardX import SummaryWriter


DEFAULT_ENV_NAME = "PongNoFrameskip-v4"
MEAN_REWARD_BOUND = 19


GAMMA = 0.99
BATCH_SIZE = 32
REPLAY_SIZE = 10000
LEARNING_RATE = 1e-4
SYNC_TARGET_FRAMES = 1000
REPLAY_START_SIZE = 10000

EPSILON_DECAY_LAST_FRAME = 150000
EPSILON_START = 1.0
EPSILON_FINAL = 0.01


Experience = collections.namedtuple(
    'Experience', field_names=['state', 'action', 'reward',
                               'done', 'new_state'])

#replay buffer is to break the correlation among the steps by random sampling 
class ExperienceBuffer:   #we need a buffer to store the experinces, we need to perform methods: appending, sampling and measuring the lengh
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)

    def __len__(self):
        return len(self.buffer)

    def append(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        s, a, r, dones, sp = zip(*[self.buffer[indx] for indx in indices])   #The experience in the buffer are ziped (one trajectory per experience) then we need to unzip them to have separted array pf states, actions, ...
        return np.array(s), np.array(a), \
               np.array(r, dtype=np.float32), \
               np.array(dones, dtype=np.uint8), \
               np.array(sp)


class Agent:
    def __init__(self, env, exp_buffer):
        self.env = env
        self.exp_buffer = exp_buffer
        self._reset()

    def _reset(self):
        self.state = self.env.reset()       
        self.total_reward = 0.0

    @torch.no_grad()   
    #tells PyTorch to not calculate the gradients, and the program explicitly uses it here (as with most neural networks) 
    #in order to not update the gradients when it is updating the weights as that would affect the back propagation.
    def play(self, net, epsilon = 0.0, device = "cpu"):   #play and fills the buffer
            
            done_reward = None                                  #??
            if np.random.random() < epsilon:
                action = self.env.action_space.sample()
            else:
                state_np = np.array([self.state], copy=False)    #??
                state_vtf = torch.tensor(state_np).to(device)    #??
                q_act_val = net(state_vtf)
                _, act = torch.max(q_act_val, dim=1)             #??  #why do not use squeez
                action = int(act.item())                         #??

            next_s, reward, is_done, _ = self.env.step(action)
            self.total_reward += reward
            self.exp_buffer.append(Experience(self.state, action, reward, is_done, next_s))

            if is_done:
                done_reward = self.total_reward
                self._reset()

            return done_reward   #what if not done??    


def cal_loss(batch, net, target_net, device = "cpu"):

    states, actions, rewards, is_dones, nxt_states = batch

    states_vtf = torch.tensor(np.array(states, copy=False)).to(device)
    nxt_states_vtf = torch.tensor(np.array(nxt_states, copy=False)).to(device)

    actions_vtf = torch.tensor(actions).to(device)
    rewards_vtf = torch.tensor(rewards).to(device)

    done_mask = torch.BoolTensor(is_dones).to(device)

    state_action_value = net(states_vtf).gather(1, actions_vtf.unsqueeze(-1)).squeeze(1)   #difficult to understand

    with torch.no_grad():    #tells PyTorch to not calculate the gradients
        next_state_value = target_net(nxt_states_vtf).max(1)[0]
        next_state_value[done_mask] = 0.0  #if the next state is the last episode then there is no next state value
        next_state_value = next_state_value.detach()  #?? why still having the with .... ?

    expected_state_action_value = rewards_vtf + GAMMA*next_state_value

    return nn.MSELoss() (state_action_value, expected_state_action_value)    



if __name__ == "__main__":

#These are command line arguments. This enables us to enable cuda and train on environments that differ from the default.
    parser = argparse.ArgumentParser()
    parser.add_argument("--cuda", default=False,
                        action="store_true", help="Enable cuda")
    parser.add_argument("--env", default=DEFAULT_ENV_NAME,
                        help="Name of the environment, default=" +
                             DEFAULT_ENV_NAME)
    args = parser.parse_args()
    device = torch.device("cuda" if args.cuda else "cpu")        

    env = wrappers.make_env(args.env)
    
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)

    writer = SummaryWriter(comment="-" + args.env)

    buffer = ExperienceBuffer(REPLAY_SIZE)
    agent = Agent(env, buffer)

    epsilon = EPSILON_START

    
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)
    total_rewards = []
    frame_idx = 0          #since each iteration is acurully a frame of the game
    ts_frame = 0
    ts = time.time()
    best_m_reward = None

    while True:
        frame_idx += 1
        epsilon = max(EPSILON_FINAL, EPSILON_START-frame_idx/EPSILON_DECAY_LAST_FRAME)

        reward = agent.play(net, epsilon, device=device)   #using training network
        if reward is not None:   #meaning the experience comes to end the episode (step is_done true and returns the total rewards)
            total_rewards.append(reward)

            speed = (frame_idx - ts_frame) / (time.time() - ts)  #number of frames divided by time to measure speed till we get to final episode
            ts_frame = frame_idx
            ts = time.time()

            reward_m = np.mean(total_rewards[-100:])

            print("%d: done %d games, reward %.3f, ""eps %.2f, speed %.2f f/s" % (
                  frame_idx, len(total_rewards), reward_m, epsilon, speed))
            
            writer.add_scalar("epsilon", epsilon, frame_idx)
            writer.add_scalar("speed", speed, frame_idx)
            writer.add_scalar("reward_100", reward_m, frame_idx)
            writer.add_scalar("reward", reward, frame_idx)

            if best_m_reward is None or best_m_reward < reward_m:
                torch.save(net.state_dict(), args.env +   
                           "-best_%.0f.dat" % reward_m)   #save the model parameters
                
                if best_m_reward is not None:
                    print("Best reward updated %.3f -> %.3f" % (best_m_reward, reward_m))
                    best_m_reward = reward_m

            if reward_m > MEAN_REWARD_BOUND:
                print("Solved in %d frames!" % frame_idx)
                break

        if len(buffer) < REPLAY_SIZE:
            continue  #go up

        if frame_idx % SYNC_TARGET_FRAMES ==0:         
            tgt_net.load_state_dict(net.state_dict())

        optimizer.zero_grad()
        
        batch = ExperienceBuffer.sample(BATCH_SIZE)
        loss = cal_loss(batch, net, tgt_net, device=device)

        loss.backward()
        optimizer.step()

    writer.close()    






        


